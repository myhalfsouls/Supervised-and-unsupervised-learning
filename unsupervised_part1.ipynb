{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533facb8-70d1-4bd7-bee7-f5b36dff565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.base import ClusterMixin\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09620cd4-ddfd-4938-b57a-d3d105071f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "seed = 11\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8202fc49-65db-413f-8a5d-d96d8f8fbb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['bean', 'fruit']\n",
    "\n",
    "# Load bean dataset\n",
    "bean = pd.read_excel('.//datasets//bean//Dry_Bean_Dataset.xlsx')\n",
    "\n",
    "X1 = bean.iloc[:, :-1]\n",
    "y1 = bean.iloc[:, -1]\n",
    "\n",
    "encoder = preprocessing.LabelEncoder().fit(y1)\n",
    "y1 = encoder.transform(y1)\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=seed, stratify=y1)\n",
    "\n",
    "scaler1 = preprocessing.StandardScaler().fit(X1_train)\n",
    "X1_train = scaler1.transform(X1_train)\n",
    "X1_test = scaler1.transform(X1_test)\n",
    "\n",
    "X1_train = X1_train.astype(np.float32)\n",
    "X1_test = X1_test.astype(np.float32)\n",
    "y1_train = y1_train.astype(np.int64)\n",
    "y1_test = y1_test.astype(np.int64)\n",
    "\n",
    "\n",
    "\n",
    "# fruit dataset\n",
    "fruit = pd.read_excel('.//datasets//fruit//fruit.xlsx')\n",
    "\n",
    "X2 = fruit.iloc[:, :-1]\n",
    "y2 = fruit.iloc[:, -1]\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "encoder.fit(y2)\n",
    "y2 = encoder.transform(y2)\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=seed, stratify=y2)\n",
    "\n",
    "scaler2 = preprocessing.StandardScaler().fit(X2_train)\n",
    "X2_train = scaler2.transform(X2_train)\n",
    "X2_test = scaler2.transform(X2_test)\n",
    "\n",
    "X2_train = X2_train.astype(np.float32)\n",
    "X2_test = X2_test.astype(np.float32)\n",
    "y2_train = y2_train.astype(np.int64)\n",
    "y2_test = y2_test.astype(np.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf30feac-3d58-45c0-b768-62a42b982203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data correlation matrices\n",
    "# the following code is adapted from https://seaborn.pydata.org/examples/many_pairwise_correlations.html\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "d = pd.DataFrame(data=X1_train, columns=bean.columns[:-1])\n",
    "corr1 = d.corr()\n",
    "mask = np.triu(np.ones_like(corr1, dtype=bool))\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "sns.heatmap(corr1, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax, xticklabels=True, yticklabels=True)\n",
    "ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "# plt.savefig('data_correlations1.png', dpi=500, bbox_inches='tight')\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "d = pd.DataFrame(data=X2_train, columns=fruit.columns[:-1])\n",
    "corr2 = d.corr()\n",
    "mask = np.triu(np.ones_like(corr2, dtype=bool))\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "sns.heatmap(corr2, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax, xticklabels=True, yticklabels=True)\n",
    "ax.tick_params(labelsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('data_correlations2.png', dpi=500, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf819704-8611-499c-8d13-836571f339fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze which features are redudant based on correlation matrix\n",
    "# the following code is adapted from https://github.com/adityav95/variable_reduction_correlation/blob/master/variable_reduction_by_correlation.ipynb\n",
    "\n",
    "corr_coeff = 0.8\n",
    "corr_matrix = corr1.abs()\n",
    "corr_mean = corr_matrix.mean()\n",
    "features_drop_list = [] # This will contain the list of features to be dropped\n",
    "features_index_drop_list = [] # This will contain the index of features to be dropped as per df_input\n",
    "corr_matrix = abs(corr_matrix)\n",
    "\n",
    "# Selecting features to be dropped (Using two for loops that runs on one triangle of the corr_matrix to avoid checking the correlation of a variable with itself)\n",
    "for i in range(corr_matrix.shape[0]):\n",
    "    for j in range(i+1,corr_matrix.shape[0]):\n",
    "\n",
    "        # The following if statement checks if each correlation value is higher than threshold (or equal) and also ensures the two columns have NOT been dropped already.  \n",
    "        if corr_matrix.iloc[i,j]>=corr_coeff and i not in features_index_drop_list and j not in features_index_drop_list:\n",
    "        \n",
    "            # The following if statement checks which of the 2 variables with high correlation has a lower correlation with target and then drops it. If equal we can drop any and it drops the first one (This is arbitrary)\n",
    "            if corr_mean[corr_matrix.columns[i]] >= corr_mean[corr_matrix.columns[j]]:\n",
    "                features_drop_list.append(corr_matrix.columns[i])\t# Name of variable that needs to be dropped appended to list\n",
    "                features_index_drop_list.append(i)\t# Index of variable that needs to be dropped appended to list. This is used to not check for the same variables repeatedly\n",
    "            else:\n",
    "                features_drop_list.append(corr_matrix.columns[j])\n",
    "                features_index_drop_list.append(j)\n",
    "print('Bean dataset: redudant features are: {}'.format(features_drop_list))\n",
    "print('Bean dataset: # of redudant features is: {}'.format(len(features_drop_list)))\n",
    "print('Bean dataset: # of remaining features is: {}'.format(X1_train.shape[1] - len(features_drop_list)))\n",
    "\n",
    "\n",
    "corr_matrix = corr2.abs()\n",
    "corr_mean = corr_matrix.mean()\n",
    "features_drop_list = [] # This will contain the list of features to be dropped\n",
    "features_index_drop_list = [] # This will contain the index of features to be dropped as per df_input\n",
    "corr_matrix = abs(corr_matrix)\n",
    "\n",
    "# Selecting features to be dropped (Using two for loops that runs on one triangle of the corr_matrix to avoid checking the correlation of a variable with itself)\n",
    "for i in range(corr_matrix.shape[0]):\n",
    "    for j in range(i+1,corr_matrix.shape[0]):\n",
    "\n",
    "        # The following if statement checks if each correlation value is higher than threshold (or equal) and also ensures the two columns have NOT been dropped already.  \n",
    "        if corr_matrix.iloc[i,j]>=corr_coeff and i not in features_index_drop_list and j not in features_index_drop_list:\n",
    "        \n",
    "            # The following if statement checks which of the 2 variables with high correlation has a lower correlation with target and then drops it. If equal we can drop any and it drops the first one (This is arbitrary)\n",
    "            if corr_mean[corr_matrix.columns[i]] >= corr_mean[corr_matrix.columns[j]]:\n",
    "                features_drop_list.append(corr_matrix.columns[i])\t# Name of variable that needs to be dropped appended to list\n",
    "                features_index_drop_list.append(i)\t# Index of variable that needs to be dropped appended to list. This is used to not check for the same variables repeatedly\n",
    "            else:\n",
    "                features_drop_list.append(corr_matrix.columns[j])\n",
    "                features_index_drop_list.append(j)\n",
    "print('Fruit dataset: redudant features are: {}'.format(features_drop_list))\n",
    "print('Fruit dataset: # of redudant features is: {}'.format(len(features_drop_list)))\n",
    "print('Fruit dataset: # of remaining features is: {}'.format(X2_train.shape[1] - len(features_drop_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc754da-841c-4310-b0c4-0c9fb34c359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyzing clusters\n",
    "\n",
    "for i in range(2):\n",
    "    cluster_results_ch = {}\n",
    "    cluster_results_time = {}\n",
    "    \n",
    "    if i == 0:\n",
    "        X_train = X1_train\n",
    "        y_train = y1_train\n",
    "        X_test = X1_test\n",
    "        y_test = y1_test\n",
    "        file = 'set1'\n",
    "    else:\n",
    "        X_train = X2_train\n",
    "        y_train = y2_train\n",
    "        X_test = X2_test\n",
    "        y_test = y2_test\n",
    "        file = 'set2'\n",
    "    \n",
    "    k_max = 10\n",
    "\n",
    "    for cov in ['full', 'tied', 'diag', 'spherical']:\n",
    "        silh_list_gmm_cov1 = []\n",
    "        silh_list_gmm_cov2 = []\n",
    "        silh_list_gmm_cov3 = []\n",
    "        for k in tqdm(range(2, k_max+1), desc='Running GMM clusters on '+datasets[i]+' dataset'):\n",
    "            gmm = GaussianMixture(n_components=k, random_state=seed, max_iter=500, n_init=100, covariance_type=cov, tol=0.0001)\n",
    "            t1 = time.time()\n",
    "            gmm.fit(X_train)\n",
    "            t2 = time.time()\n",
    "            y_train_pred = gmm.predict(X_train)\n",
    "            silh_list_gmm_cov1.append(metrics.calinski_harabasz_score(X_train, y_train_pred))\n",
    "            silh_list_gmm_cov3.append(t2-t1)\n",
    "        cluster_results_ch['GMM_'+cov] = silh_list_gmm_cov1\n",
    "        cluster_results_time['GMM_'+cov] = silh_list_gmm_cov3\n",
    "            \n",
    "    \n",
    "    silh_list_kmean1 = []\n",
    "    silh_list_kmean3 = []\n",
    "    for k in tqdm(range(2, k_max+1), desc='Running k-means clusters on '+datasets[i]+' dataset'):\n",
    "        kmean = KMeans(n_clusters=k, max_iter=500, random_state=seed, n_init=500)\n",
    "        t1 = time.time()\n",
    "        kmean.fit(X_train)\n",
    "        t2 = time.time()\n",
    "        y_train_pred = kmean.predict(X_train)\n",
    "        silh_list_kmean1.append(metrics.calinski_harabasz_score(X_train, y_train_pred))\n",
    "        silh_list_kmean3.append(t2-t1)\n",
    "    cluster_results_ch['KM'] = silh_list_kmean1\n",
    "    cluster_results_time['KM'] = silh_list_kmean3\n",
    "\n",
    "    cluster_results_ch = pd.DataFrame(cluster_results_ch).to_excel('cluster_ch_'+file+'.xlsx')\n",
    "    cluster_results_time = pd.DataFrame(cluster_results_time).to_excel('cluster_time_'+file+'.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc863ea9-9329-43a7-8f83-e4cc360889fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering results\n",
    "\n",
    "k_max = 10\n",
    "set1 = pd.read_excel('cluster_ch_set1.xlsx', index_col=0)\n",
    "set2 = pd.read_excel('cluster_ch_set2.xlsx', index_col=0)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5.5))\n",
    "ax = ax.flatten()\n",
    "ks = list(range(2, k_max+1))\n",
    "\n",
    "ax[0].plot(ks, set1['GMM_full'], label='GMM full')\n",
    "ax[0].plot(ks, set1['KM'], label='K-Means')\n",
    "ax[0].plot(ks, set1['GMM_tied'], '--', label='GMM tied', alpha=0.4)\n",
    "ax[0].plot(ks, set1['GMM_diag'], '--', label='GMM diag', alpha=0.4)\n",
    "ax[0].plot(ks, set1['GMM_spherical'], '--', label='GMM spherical', alpha=0.4)\n",
    "ax[0].set_xlabel('Number of clusters')\n",
    "ax[0].set_ylabel('Calinski-Harabasz score')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(ks, set2['GMM_full'], label='GMM full')\n",
    "ax[1].plot(ks, set2['KM'], label='K-Means')\n",
    "ax[1].plot(ks, set2['GMM_tied'], '--', label='GMM tied', alpha=0.4)\n",
    "ax[1].plot(ks, set2['GMM_diag'], '--', label='GMM diag', alpha=0.4)\n",
    "ax[1].plot(ks, set2['GMM_spherical'], '--', label='GMM spherical', alpha=0.4)\n",
    "ax[1].set_xlabel('Number of clusters')\n",
    "ax[1].set_ylabel('Calinski-Harabasz score')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.tight_layout()\n",
    "# plt.savefig('Clustering_k.png', bbox_inches='tight', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e448903-99f1-4c97-858e-9b9d3883ae35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bean dataset 2 vs 5 clusters\n",
    "X_train = X1_train\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 5.5))\n",
    "ax = ax.flatten()\n",
    "k = [2, 5]\n",
    "\n",
    "for i in range(len(k)):\n",
    "    km = KMeans(n_clusters=k[i], max_iter=500, random_state=seed, n_init=500)\n",
    "    km.fit(X_train)\n",
    "    y_train_pred = km.predict(X_train)\n",
    "    \n",
    "    # following code adapted from https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "    silhouette_avg = metrics.silhouette_score(X_train, y_train_pred)\n",
    "    sample_silhouette_values = metrics.silhouette_samples(X_train, y_train_pred)\n",
    "        \n",
    "    y_lower = 10\n",
    "    for idx in range(k[i]):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[y_train_pred == idx]\n",
    "    \n",
    "        ith_cluster_silhouette_values.sort()\n",
    "    \n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "    \n",
    "        color = cm.nipy_spectral(float(idx) / k[i])\n",
    "        ax[i].fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "    \n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax[i].text(-0.05, y_lower + 0.5 * size_cluster_i, str(idx))\n",
    "    \n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "    \n",
    "    ax[i].set_xlabel(\"Silhouette coefficient\")\n",
    "    ax[i].set_ylabel(\"Cluster label\")\n",
    "    \n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax[i].axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    \n",
    "    ax[i].set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax[i].set_xticks([-0.2, 0, 0.2, 0.4, 0.6, 0.8])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train = X2_train\n",
    "\n",
    "km = KMeans(n_clusters=3, max_iter=500, random_state=seed, n_init=500)\n",
    "km.fit(X_train)\n",
    "y_train_pred = km.predict(X_train)\n",
    "\n",
    "# following code adapted from https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "silhouette_avg = metrics.silhouette_score(X_train, y_train_pred)\n",
    "sample_silhouette_values = metrics.silhouette_samples(X_train, y_train_pred)\n",
    "    \n",
    "y_lower = 10\n",
    "for idx in range(3):\n",
    "    # Aggregate the silhouette scores for samples belonging to\n",
    "    # cluster i, and sort them\n",
    "    ith_cluster_silhouette_values = sample_silhouette_values[y_train_pred == idx]\n",
    "\n",
    "    ith_cluster_silhouette_values.sort()\n",
    "\n",
    "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "\n",
    "    color = cm.nipy_spectral(float(idx) / 3)\n",
    "    ax[2].fill_betweenx(\n",
    "        np.arange(y_lower, y_upper),\n",
    "        0,\n",
    "        ith_cluster_silhouette_values,\n",
    "        facecolor=color,\n",
    "        edgecolor=color,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "\n",
    "    # Label the silhouette plots with their cluster numbers at the middle\n",
    "    ax[2].text(-0.05, y_lower + 0.5 * size_cluster_i, str(idx))\n",
    "\n",
    "    # Compute the new y_lower for next plot\n",
    "    y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "ax[2].set_xlabel(\"Silhouette coefficient\")\n",
    "ax[2].set_ylabel(\"Cluster label\")\n",
    "\n",
    "# The vertical line for average silhouette score of all the values\n",
    "ax[2].axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "ax[2].set_yticks([])  # Clear the yaxis labels / ticks\n",
    "ax[2].set_xticks([-0.2, 0, 0.2, 0.4, 0.6, 0.8])\n",
    "\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.tight_layout()\n",
    "# plt.savefig('Clustering_bean_k.png', bbox_inches='tight', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d13228-2068-44e3-b475-859cc2b98cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering visualization with tsne\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12, 10.5))\n",
    "ax = ax.flatten()\n",
    "k = {'bean_gmm': 5, 'bean_km': 5, 'fruit_gmm': 3, 'fruit_km': 3}\n",
    "perplexities = [50, 50]\n",
    "\n",
    "cluster_labels = []\n",
    "for i in range(2):\n",
    "    if i == 0:\n",
    "        X_train = X1_train\n",
    "        y_train = y1_train\n",
    "        cov = 'full'\n",
    "    else:\n",
    "        X_train = X2_train\n",
    "        y_train = y2_train\n",
    "        cov = 'tied'\n",
    "    \n",
    "    gmm = GaussianMixture(n_components=k[datasets[i]+'_gmm'], random_state=seed, max_iter=500, n_init=25, covariance_type='full', tol=0.0001)\n",
    "    gmm.fit(X_train)\n",
    "    y_train_pred = gmm.predict(X_train)\n",
    "    vms = metrics.homogeneity_completeness_v_measure(y_train, y_train_pred)\n",
    "    print('{} dataset with GMM clustering, the homogeneity is {}'.format(datasets[i], vms[0]))\n",
    "    print('{} dataset with GMM clustering, the completeness is {}'.format(datasets[i], vms[1]))\n",
    "    print('{} dataset with GMM clustering, the V measure is {}'.format(datasets[i], vms[2]))\n",
    "\n",
    "    ordering = np.argsort(np.unique(y_train_pred, return_counts=True)[1])\n",
    "\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexities[i])\n",
    "    proj = tsne.fit_transform(X_train)\n",
    "\n",
    "    for j in ordering:\n",
    "        ax[2*i].plot(proj[y_train_pred==j, 0], proj[y_train_pred==j, 1], '.', label='cluster '+str(j))\n",
    "    ax[2*i].set_xlabel('TC1')\n",
    "    ax[2*i].set_ylabel('TC2')\n",
    "\n",
    "    cluster_label = np.zeros((k[datasets[i]+'_gmm'], len(np.unique(y_train))))\n",
    "    for cl in range(cluster_label.shape[0]):\n",
    "        for lb in range(cluster_label.shape[1]):\n",
    "            cluster_label[cl, lb] = np.count_nonzero((y_train_pred==ordering[cl]) & (y_train==lb))\n",
    "    cluster_label = pd.DataFrame(cluster_label, index=np.arange(k[datasets[i]+'_gmm']), columns=np.unique(y_train))\n",
    "    cluster_labels.append(cluster_label)\n",
    "\n",
    "    km = KMeans(n_clusters=k[datasets[i]+'_km'], max_iter=500, random_state=seed, n_init=500)\n",
    "    km.fit(X_train)\n",
    "    y_train_pred = km.predict(X_train)\n",
    "    vms = metrics.homogeneity_completeness_v_measure(y_train, y_train_pred)\n",
    "    print('{} dataset with KM clustering, the homogeneity is {}'.format(datasets[i], vms[0]))\n",
    "    print('{} dataset with KM clustering, the completeness is {}'.format(datasets[i], vms[1]))\n",
    "    print('{} dataset with KM clustering, the V measure is {}'.format(datasets[i], vms[2]))\n",
    "    \n",
    "    ordering = np.argsort(np.unique(y_train_pred, return_counts=True)[1])\n",
    "    \n",
    "    for j in ordering:\n",
    "        ax[2*i+1].plot(proj[y_train_pred==j, 0], proj[y_train_pred==j, 1], '.', label='cluster '+str(j))\n",
    "    ax[2*i+1].set_xlabel('TC1')\n",
    "    ax[2*i+1].set_ylabel('TC2')\n",
    "\n",
    "    cluster_label = np.zeros((k[datasets[i]+'_km'], len(np.unique(y_train))))\n",
    "    for cl in range(cluster_label.shape[0]):\n",
    "        for lb in range(cluster_label.shape[1]):\n",
    "            cluster_label[cl, lb] = np.count_nonzero((y_train_pred==ordering[cl]) & (y_train==lb))\n",
    "    cluster_label = pd.DataFrame(cluster_label, index=np.arange(k[datasets[i]+'_km']), columns=np.unique(y_train))\n",
    "    cluster_labels.append(cluster_label)\n",
    "    \n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.tight_layout()\n",
    "# plt.savefig('Clustering_vis.png', bbox_inches='tight', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f976576-7a76-4c1b-9612-d8d52f11af2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(12, 10.5))\n",
    "ax = ax.flatten()\n",
    "\n",
    "ax[0] = sns.heatmap(cluster_labels[0], cmap=cmap, center=0, square=True, linewidths=.5, vmin=0, vmax=2500, cbar_kws={\"shrink\": .5}, ax=ax[0])\n",
    "ax[0].set_xlabel('Labels')\n",
    "ax[0].set_ylabel('Clusters')\n",
    "\n",
    "ax[1] = sns.heatmap(cluster_labels[1], cmap=cmap, center=0, square=True, linewidths=.5, vmin=0, vmax=2500, cbar_kws={\"shrink\": .5}, ax=ax[1])\n",
    "ax[1].set_xlabel('Labels')\n",
    "ax[1].set_ylabel('Clusters')\n",
    "\n",
    "ax[2] = sns.heatmap(cluster_labels[2], cmap=cmap, center=0, square=True, linewidths=.5, vmin=0, vmax=200, cbar_kws={\"shrink\": .5}, ax=ax[2])\n",
    "ax[2].set_xlabel('Labels')\n",
    "ax[2].set_ylabel('Clusters')\n",
    "\n",
    "ax[3] = sns.heatmap(cluster_labels[3], cmap=cmap, center=0, square=True, linewidths=.5, vmin=0, vmax=200, cbar_kws={\"shrink\": .5}, ax=ax[3])\n",
    "ax[3].set_xlabel('Labels')\n",
    "ax[3].set_ylabel('Clusters')\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.tight_layout()\n",
    "# plt.savefig('Clustering_results.png', bbox_inches='tight', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b6d2e8-04b4-4785-beda-96a1621cb08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA analysis\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5.5))\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i in range(2):\n",
    "    if i == 0:\n",
    "        X_train = X1_train\n",
    "    else:\n",
    "        X_train = X2_train\n",
    "\n",
    "    pca = PCA(n_components=None, random_state=seed)\n",
    "    pca.fit(X_train)\n",
    "    cumvar = np.cumsum(pca.explained_variance_ratio_)\n",
    "    ln1 = ax[i].plot(list(range(1, X_train.shape[1] + 1)), cumvar, label='Cumulative variance')\n",
    "\n",
    "    thresh = 0.95\n",
    "    comp_best = np.argmax(cumvar > thresh)\n",
    "    \n",
    "    ax[i].axhline(y=cumvar[comp_best], color='grey', linestyle='--')\n",
    "    ax[i].axvline(x=comp_best+1, color='grey', linestyle='--')\n",
    "    ax[i].set_xlabel('Number of PCs')\n",
    "    ax[i].set_ylabel('Cumulative variance (%)')\n",
    "    \n",
    "    if i == 0:\n",
    "        ax[i].set_xticks([0, 2, 4, 6, 8, 10, 12, 14, 16])\n",
    "        ax[i].set_xlim([-0.5, 16.5])\n",
    "        text_x = comp_best + 1.5\n",
    "        text_y = thresh - 0.01\n",
    "    else:\n",
    "        ax[i].set_xticks([0, 5, 10, 15, 20, 25, 30, 35])\n",
    "        ax[i].set_xlim([-0.5, 36])\n",
    "        text_x = comp_best + 2\n",
    "        text_y = thresh - 0.04\n",
    "    ax[i].text(text_x, text_y, 'Components = '+str(comp_best+1), color = 'black')\n",
    "    \n",
    "    ax2 = ax[i].twinx()\n",
    "    \n",
    "    error = []\n",
    "    for k in range(1, X_train.shape[1]+1):\n",
    "        pca = PCA(n_components=k, random_state=seed)\n",
    "        pca.fit(X_train)\n",
    "        X_train_trans = pca.transform(X_train)\n",
    "        X_train_recons = X_train_trans @ pca.components_\n",
    "        error.append(metrics.mean_squared_error(X_train, X_train_recons))\n",
    "    ln2 = ax2.plot(list(range(1, X_train.shape[1] + 1)), error, color='orange', label='Reconstruction RMSE')\n",
    "    ax2.set_ylabel('Reconstruction RMSE')\n",
    "\n",
    "lns = ln1 + ln2\n",
    "labs = [l.get_label() for l in lns]\n",
    "fig.legend(lns, labs, bbox_to_anchor =(0.5, -0.07), loc='lower center', ncol=2)\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.tight_layout()\n",
    "# plt.savefig('PCA_k.png', bbox_inches='tight', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1493d89b-ed2d-4803-b6c4-e56c96710a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA analysis 2\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 7))\n",
    "ax = ax.flatten()\n",
    "\n",
    "X_train = X1_train\n",
    "pca = PCA(n_components=5, random_state=seed)\n",
    "pca.fit(X_train)\n",
    "var = pca.explained_variance_ratio_\n",
    "cols = ['PC{}'.format(i) for i in range(1, 6)]\n",
    "pca_matrix = pd.DataFrame(pca.components_.T * var, index=X1.columns, columns=cols)\n",
    "ax[0] = sns.heatmap(pca_matrix, cmap=cmap, center=0, square=True, linewidths=.5, vmin=-0.2, vmax=0.2, cbar_kws={\"shrink\": .5}, ax=ax[0],  xticklabels=True, yticklabels=True)\n",
    "\n",
    "X_train = X2_train\n",
    "pca = PCA(n_components=8, random_state=seed)\n",
    "pca.fit(X_train)\n",
    "var = pca.explained_variance_ratio_\n",
    "cols = ['PC{}'.format(i) for i in range(1, 9)]\n",
    "pca_matrix = pd.DataFrame(pca.components_.T * var, index=X2.columns, columns=cols)\n",
    "ax[1] = sns.heatmap(pca_matrix, cmap=cmap, center=0, square=True, linewidths=.5, vmin=-0.11, vmax=0.11, cbar_kws={\"shrink\": .5}, ax=ax[1],  xticklabels=True, yticklabels=True)\n",
    "ax[1].tick_params(labelsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('PCA_analysis.png', dpi=500, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f172f928-f060-46ed-a031-80e34d50db76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICA analysis\n",
    "seed = 7\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5.5))\n",
    "ax = ax.flatten()\n",
    "comp_best = [3, 4]\n",
    "\n",
    "for i in range(2):\n",
    "    if i == 0:\n",
    "        X_train = X1_train\n",
    "    else:\n",
    "        X_train = X2_train\n",
    "    max_iter = 10000\n",
    "    ica = FastICA(n_components=X_train.shape[1], random_state=seed, max_iter=max_iter)\n",
    "    ica.fit(X_train)\n",
    "    X_train_trans = ica.transform(X_train)\n",
    "    kurtosis = pd.DataFrame(X_train_trans).kurt(axis=0).abs()\n",
    "    kurtosis = pd.concat([pd.DataFrame(kurtosis, columns=['kurt']), pd.DataFrame(ica.components_)], axis=1)\n",
    "    kurtosis = kurtosis.sort_values(by='kurt', ascending=False)\n",
    "\n",
    "    cumkurt = np.cumsum(kurtosis['kurt'].to_numpy())\n",
    "    cumkurt = cumkurt / np.sum(kurtosis['kurt'])\n",
    "\n",
    "    thresh = 0.95\n",
    "    comp_best = np.argmax(cumkurt > thresh)\n",
    "\n",
    "    error = []\n",
    "    for k in range(1, X_train.shape[1]+1):\n",
    "        best_components = kurtosis.iloc[:k, 1:]\n",
    "        best_idx = best_components.index\n",
    "        X_train_trans_sel = X_train_trans.copy()\n",
    "        to_remove = np.delete(np.arange(X_train.shape[1]), best_idx)\n",
    "        X_train_trans_sel[:, to_remove] = 0\n",
    "        X_train_recons = ica.inverse_transform(X_train_trans_sel)\n",
    "        error.append(metrics.mean_squared_error(X_train, X_train_recons))\n",
    "\n",
    "    if i == 0:\n",
    "        ax[i].set_xticks([0, 2, 4, 6, 8, 10, 12, 14, 16])\n",
    "        ax[i].set_xlim([-0.5, 16.5])\n",
    "        text_x = comp_best + 1.5\n",
    "        text_y = cumkurt[comp_best] - 0.06  \n",
    "    else:\n",
    "        ax[i].set_xticks([0, 5, 10, 15, 20, 25, 30, 35])\n",
    "        ax[i].set_xlim([-0.5, 36])\n",
    "        text_x = comp_best + 2.5\n",
    "        text_y = cumkurt[comp_best] - 0.06    \n",
    "\n",
    "    ax[i].axvline(x=comp_best+1, color='grey', linestyle='--')\n",
    "    \n",
    "    ln1 = ax[i].bar(list(range(1, X_train.shape[1]+1)), kurtosis['kurt'].tolist(), label='Kurtosis')\n",
    "    ax[i].set_xlabel('IC or Number of ICs')\n",
    "    ax[i].set_ylabel('Component kurtosis')\n",
    "    ax2 = ax[i].twinx()\n",
    "    ax2.axhline(y=cumkurt[comp_best], color='grey', linestyle='--')\n",
    "    ax2.text(text_x, text_y, 'Components = '+str(comp_best), color = 'black')\n",
    "    ln2 = ax2.plot(list(range(1, X_train.shape[1]+1)), error, color='orange', label='Reconstruction RMSE', linewidth=2)\n",
    "    ln3 = ax2.plot(list(range(1, X_train.shape[1]+1)), cumkurt, color='#d62728', label='Cumulative kurtosis', linewidth=2)\n",
    "    ax2.set_ylabel('Reconstruction RMSE or Cumulative kurtosis')\n",
    "\n",
    "lns = [ln1] + ln2 + ln3\n",
    "labs = [l.get_label() for l in lns]\n",
    "fig.legend(lns, labs, bbox_to_anchor =(0.5, -0.07), loc='lower center', ncol=3)\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.tight_layout()\n",
    "# plt.savefig('ICA_k.png', bbox_inches='tight', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7378cc1-17ad-4052-8dd3-6ec830e305dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICA analysis 2\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 7))\n",
    "ax = ax.flatten()\n",
    "\n",
    "X_train = X1_train\n",
    "ica = FastICA(n_components=None, random_state=seed)\n",
    "ica.fit(X_train)\n",
    "X_train_trans = ica.transform(X_train)\n",
    "kurtosis = pd.DataFrame(X_train_trans).kurt(axis=0).abs()\n",
    "kurtosis = pd.concat([pd.DataFrame(kurtosis, columns=['kurt']), pd.DataFrame(ica.components_)], axis=1)\n",
    "kurtosis = kurtosis.sort_values(by='kurt', ascending=False)\n",
    "best_components = kurtosis.iloc[:8, 1:].to_numpy()\n",
    "best_components = (best_components.T / np.linalg.norm(best_components, axis=1)).T\n",
    "best_kurtosis = kurtosis.iloc[:8, 0].to_numpy() / np.sum(kurtosis['kurt'])\n",
    "\n",
    "cols = ['IC{}'.format(i) for i in range(1, 9)]\n",
    "ica_matrix = best_components.T * best_kurtosis\n",
    "ica_matrix = pd.DataFrame(ica_matrix, index=X1.columns, columns=cols)\n",
    "ax[0] = sns.heatmap(ica_matrix, cmap=cmap, center=0, square=True, linewidths=.5, vmin=-0.22, vmax=0.22, cbar_kws={\"shrink\": .5}, ax=ax[0],  xticklabels=True, yticklabels=True)\n",
    "ax[0].tick_params('x', labelrotation=90)\n",
    "\n",
    "X_train = X2_train\n",
    "ica = FastICA(n_components=None, random_state=seed)\n",
    "ica.fit(X_train)\n",
    "X_train_trans = ica.transform(X_train)\n",
    "kurtosis = pd.DataFrame(X_train_trans).kurt(axis=0).abs()\n",
    "kurtosis = pd.concat([pd.DataFrame(kurtosis, columns=['kurt']), pd.DataFrame(ica.components_)], axis=1)\n",
    "kurtosis = kurtosis.sort_values(by='kurt', ascending=False)\n",
    "best_components = kurtosis.iloc[:17, 1:].to_numpy()\n",
    "best_components = (best_components.T / np.linalg.norm(best_components, axis=1)).T\n",
    "best_kurtosis = kurtosis.iloc[:17, 0].to_numpy() / np.sum(kurtosis['kurt'])\n",
    "\n",
    "cols = ['IC{}'.format(i) for i in range(1, 18)]\n",
    "ica_matrix = best_components.T * best_kurtosis\n",
    "ica_matrix = pd.DataFrame(ica_matrix, index=X2.columns, columns=cols)\n",
    "ax[1] = sns.heatmap(ica_matrix, cmap=cmap, center=0, square=True, linewidths=.5, vmin=-0.15, vmax=0.15, cbar_kws={\"shrink\": .5}, ax=ax[1],  xticklabels=True, yticklabels=True)\n",
    "ax[1].tick_params(labelsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('ICA_analysis.png', dpi=500, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9636e06-5e59-4e6e-b545-f4b49963cd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [0, 7, 42, 69, 518]\n",
    "comps = [6, 14]\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5.5))\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i in range(2):\n",
    "    if i == 0:\n",
    "        X_train = X1_train\n",
    "    else:\n",
    "        X_train = X2_train\n",
    "\n",
    "    error_avg = []\n",
    "    error_std = []\n",
    "    for k in range(1, X_train.shape[1]+1):\n",
    "        error = []\n",
    "        for seed in seeds:\n",
    "            # rp = SparseRandomProjection(n_components=k, random_state=seed, compute_inverse_components=True)\n",
    "            rp = GaussianRandomProjection(n_components=k, random_state=seed, compute_inverse_components=True)\n",
    "            rp.fit(X_train)\n",
    "            X_train_trans = rp.transform(X_train)\n",
    "            X_train_recons = X_train_trans @ rp.inverse_components_.T\n",
    "            error.append(metrics.mean_squared_error(X_train, X_train_recons))\n",
    "        error_avg.append(np.mean(error))\n",
    "        error_std.append(np.std(error))  \n",
    "\n",
    "    if i == 0:\n",
    "        ax[i].set_xticks([0, 2, 4, 6, 8, 10, 12, 14, 16])\n",
    "        ax[i].set_xlim([-0.5, 16.5])\n",
    "        text_x = comps[i] + 0.5\n",
    "    else:\n",
    "        text_x = comps[i] + 1\n",
    "    text_y = 0.8\n",
    "    \n",
    "    ax[i].plot(list(range(1, X_train.shape[1]+1)), error_avg)\n",
    "    ax[i].fill_between(list(range(1, X_train.shape[1]+1)), np.array(error_avg) - np.array(error_std), np.array(error_avg) + np.array(error_std), alpha=0.1, color='darkorchid')\n",
    "    ax[i].set_xlabel('Number of RP components')\n",
    "    ax[i].set_ylabel('Reconstruction RMSE')\n",
    "    # ax[i].axvline(x=comps[i], color='grey', linestyle='--')\n",
    "    # ax[i].text(text_x, text_y, 'Components = '+str(comps[i]), color = 'black')\n",
    "    # ax[i].axhline(y=error_avg[comps[i]-1], color='grey', linestyle='--')\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.tight_layout()\n",
    "# plt.savefig('RP_k.png', bbox_inches='tight', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f5bdbd-70cd-4d71-a1dc-7bc63a15ce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cPCA = [5, 8]\n",
    "cICA = [6, 10]\n",
    "cRP = [6, 14]\n",
    "seed = 8\n",
    "for DR in ['PCA', 'ICA', 'RP']:\n",
    "    for i in range(2):\n",
    "        cluster_results_ch = {}\n",
    "        cluster_results_time = {}\n",
    "        \n",
    "        if i == 0:\n",
    "            X_train = X1_train\n",
    "            file = 'set1'\n",
    "        else:\n",
    "            X_train = X2_train\n",
    "            file = 'set2'\n",
    "\n",
    "        if DR == 'PCA':\n",
    "            DR_algo = PCA(n_components=cPCA[i], random_state=seed)\n",
    "            DR_algo.fit(X_train)\n",
    "            X_train = DR_algo.transform(X_train)\n",
    "        elif DR == 'ICA':\n",
    "            DR_algo = FastICA(n_components=cICA[i], random_state=seed, max_iter=2000)\n",
    "            DR_algo.fit(X_train)\n",
    "            \n",
    "            X_train = DR_algo.transform(X_train)\n",
    "        elif DR == 'RP':\n",
    "            DR_algo = GaussianRandomProjection(n_components=cRP[i], random_state=seed)\n",
    "            DR_algo.fit(X_train)\n",
    "            X_train = DR_algo.transform(X_train)\n",
    "\n",
    "        k_max = 10\n",
    "        \n",
    "        for cov in ['full', 'tied']:\n",
    "            silh_list_gmm_cov1 = []\n",
    "            silh_list_gmm_cov3 = []\n",
    "            for k in tqdm(range(2, k_max+1), desc='Running GMM clusters on '+datasets[i]+' dataset with '+DR+' transformation'):\n",
    "                gmm = GaussianMixture(n_components=k, random_state=seed, max_iter=500, n_init=100, covariance_type=cov, tol=0.0001)\n",
    "                t1 = time.time()\n",
    "                gmm.fit(X_train)\n",
    "                t2 = time.time()\n",
    "                y_train_pred = gmm.predict(X_train)\n",
    "                silh_list_gmm_cov1.append(metrics.calinski_harabasz_score(X_train, y_train_pred))\n",
    "                silh_list_gmm_cov3.append(t2-t1)\n",
    "            cluster_results_ch['GMM_'+cov] = silh_list_gmm_cov1\n",
    "            cluster_results_time['GMM_'+cov] = silh_list_gmm_cov3\n",
    "                \n",
    "        \n",
    "        silh_list_kmean1 = []\n",
    "        silh_list_kmean3 = []\n",
    "        for k in tqdm(range(2, k_max+1), desc='Running k-means clusters on '+datasets[i]+' dataset with '+DR+' transformation'):\n",
    "            kmean = KMeans(n_clusters=k, max_iter=500, random_state=seed, n_init=500)\n",
    "            t1 = time.time()\n",
    "            kmean.fit(X_train)\n",
    "            t2 = time.time()\n",
    "            y_train_pred = kmean.predict(X_train)\n",
    "            silh_list_kmean1.append(metrics.calinski_harabasz_score(X_train, y_train_pred))\n",
    "            silh_list_kmean3.append(t2-t1)\n",
    "        cluster_results_ch['KM'] = silh_list_kmean1\n",
    "        cluster_results_time['KM'] = silh_list_kmean3\n",
    "    \n",
    "        cluster_results_ch = pd.DataFrame(cluster_results_ch).to_excel('cluster_ch_'+DR+file+'.xlsx')\n",
    "        cluster_results_time = pd.DataFrame(cluster_results_time).to_excel('cluster_time_'+DR+file+'.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052173f9-e96f-47be-b777-9fbf23a2f9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering results\n",
    "\n",
    "k_max = 10\n",
    "PCAset1 = pd.read_excel('cluster_ch_PCAset1.xlsx', index_col=0)\n",
    "PCAset2 = pd.read_excel('cluster_ch_PCAset2.xlsx', index_col=0)\n",
    "ICAset1 = pd.read_excel('cluster_ch_ICAset1.xlsx', index_col=0)\n",
    "ICAset2 = pd.read_excel('cluster_ch_ICAset2.xlsx', index_col=0)\n",
    "RPset1 = pd.read_excel('cluster_ch_RPset1.xlsx', index_col=0)\n",
    "RPset2 = pd.read_excel('cluster_ch_RPset2.xlsx', index_col=0)\n",
    "\n",
    "fig, ax = plt.subplots(3, 2, figsize=(12, 16))\n",
    "ax = ax.flatten()\n",
    "ks = list(range(2, k_max+1))\n",
    "\n",
    "ax[0].plot(ks, PCAset1['GMM_full'], label='GMM')\n",
    "ax[0].plot(ks, PCAset1['KM'], label='K-Means')\n",
    "ax[0].set_xlabel('Number of clusters')\n",
    "ax[0].set_ylabel('Calinski-Harabasz score')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(ks, PCAset2['GMM_full'], label='GMM')\n",
    "ax[1].plot(ks, PCAset2['KM'], label='K-Means')\n",
    "ax[1].set_xlabel('Number of clusters')\n",
    "ax[1].set_ylabel('Calinski-Harabasz score')\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].plot(ks, ICAset1['GMM_full'], label='GMM')\n",
    "ax[2].plot(ks, ICAset1['KM'], label='K-Means')\n",
    "ax[2].set_xlabel('Number of clusters')\n",
    "ax[2].set_ylabel('Calinski-Harabasz score')\n",
    "ax[2].legend()\n",
    "\n",
    "ax[3].plot(ks, ICAset2['GMM_full'], label='GMM')\n",
    "ax[3].plot(ks, ICAset2['KM'], label='K-Means')\n",
    "ax[3].set_xlabel('Number of clusters')\n",
    "ax[3].set_ylabel('Calinski-Harabasz score')\n",
    "ax[3].legend()\n",
    "\n",
    "ax[4].plot(ks, RPset1['GMM_full'], label='GMM')\n",
    "ax[4].plot(ks, RPset1['KM'], label='K-Means')\n",
    "ax[4].set_xlabel('Number of clusters')\n",
    "ax[4].set_ylabel('Calinski-Harabasz score')\n",
    "ax[4].legend()\n",
    "\n",
    "ax[5].plot(ks, RPset2['GMM_full'], label='GMM')\n",
    "ax[5].plot(ks, RPset2['KM'], label='K-Means')\n",
    "ax[5].set_xlabel('Number of clusters')\n",
    "ax[5].set_ylabel('Calinski-Harabasz score')\n",
    "ax[5].legend()\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.tight_layout()\n",
    "# plt.savefig('Clustering_afterDR_k.png', bbox_inches='tight', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dee502c-021e-4bb0-af38-526ca7a774e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(6, 2, figsize=(12, 31.5))\n",
    "ax = ax.flatten()\n",
    "k_PCA_GMM = [5, 4]\n",
    "k_ICA_GMM = [6, 4]\n",
    "k_PCA_KM = [5, 3]\n",
    "k_ICA_KM = [7, 5]\n",
    "k_RP_GMM = [5, 3]\n",
    "k_RP_KM = [5, 3]\n",
    "perplexities = [70, 70]\n",
    "cPCA = [5, 8]\n",
    "cICA = [6, 10]\n",
    "cRP = [6, 14]\n",
    "\n",
    "m = 0\n",
    "cluster_labels = []\n",
    "for i in range(2):\n",
    "    for DR in ['PCA', 'ICA', 'RP']:\n",
    "        if i == 0:\n",
    "            X_train = X1_train\n",
    "            y_train = y1_train\n",
    "            if DR == 'PCA':\n",
    "                cov = 'full'\n",
    "            elif DR == 'ICA':\n",
    "                cov = 'tied'\n",
    "        else:\n",
    "            X_train = X2_train\n",
    "            y_train = y2_train\n",
    "            if DR == 'PCA':\n",
    "                cov = 'full'\n",
    "            elif DR == 'ICA':\n",
    "                cov = 'tied'\n",
    "\n",
    "        if DR == 'PCA':\n",
    "            DR_algo = PCA(n_components=cPCA[i], random_state=seed)\n",
    "            DR_algo.fit(X_train)\n",
    "            X_train = DR_algo.transform(X_train)\n",
    "            k_GMM = k_PCA_GMM\n",
    "            k_KM = k_PCA_KM\n",
    "        elif DR == 'ICA':\n",
    "            DR_algo = FastICA(n_components=cICA[i], random_state=seed, max_iter=2000)\n",
    "            DR_algo.fit(X_train)\n",
    "            X_train = DR_algo.transform(X_train)\n",
    "            \n",
    "            k_GMM = k_ICA_GMM\n",
    "            k_KM = k_ICA_KM\n",
    "\n",
    "        elif DR == 'RP':\n",
    "            DR_algo = GaussianRandomProjection(n_components=cRP[i], random_state=seed)\n",
    "            DR_algo.fit(X_train)\n",
    "            X_train = DR_algo.transform(X_train)\n",
    "            k_GMM = k_RP_GMM\n",
    "            k_KM = k_RP_KM\n",
    "        \n",
    "        gmm = GaussianMixture(n_components=k_GMM[i], random_state=seed, max_iter=500, n_init=25, covariance_type=cov, tol=0.0001)\n",
    "        gmm.fit(X_train)\n",
    "        y_train_pred = gmm.predict(X_train)\n",
    "\n",
    "        vms = metrics.homogeneity_completeness_v_measure(y_train, y_train_pred)\n",
    "        print('{} dataset after {} with GMM clustering, the homogeneity is {}'.format(datasets[i], DR, vms[0]))\n",
    "        print('{} dataset after {} with GMM clustering, the completeness is {}'.format(datasets[i], DR, vms[1]))\n",
    "        print('{} dataset after {} with GMM clustering, the V measure is {}'.format(datasets[i], DR, vms[2]))\n",
    "    \n",
    "        ordering = np.argsort(np.unique(y_train_pred, return_counts=True)[1])\n",
    "\n",
    "        cluster_label = np.zeros((k_GMM[i], len(np.unique(y_train))))\n",
    "        for cl in range(cluster_label.shape[0]):\n",
    "            for lb in range(cluster_label.shape[1]):\n",
    "                cluster_label[cl, lb] = np.count_nonzero((y_train_pred==ordering[cl]) & (y_train==lb))\n",
    "        cluster_label = pd.DataFrame(cluster_label, index=np.arange(k_GMM[i]), columns=np.unique(y_train))\n",
    "        cluster_labels.append(cluster_label)\n",
    "\n",
    "        tsne = TSNE(n_components=2, perplexity=perplexities[i])\n",
    "        proj = tsne.fit_transform(X_train)\n",
    "        \n",
    "        for j in ordering:\n",
    "            ax[m].plot(proj[y_train_pred==j, 0], proj[y_train_pred==j, 1], '.', label='cluster '+str(j))\n",
    "        ax[m].set_xlabel('TC1')\n",
    "        ax[m].set_ylabel('TC2')\n",
    "    \n",
    "        km = KMeans(n_clusters=k_KM[i], max_iter=500, random_state=seed, n_init=500)\n",
    "        km.fit(X_train)\n",
    "        y_train_pred = km.predict(X_train)\n",
    "\n",
    "        vms = metrics.homogeneity_completeness_v_measure(y_train, y_train_pred)\n",
    "        print('{} dataset after {} with KM clustering, the homogeneity is {}'.format(datasets[i], DR, vms[0]))\n",
    "        print('{} dataset after {} with KM clustering, the completeness is {}'.format(datasets[i], DR, vms[1]))\n",
    "        print('{} dataset after {} with KM clustering, the V measure is {}'.format(datasets[i], DR, vms[2]))\n",
    "        \n",
    "        ordering = np.argsort(np.unique(y_train_pred, return_counts=True)[1])\n",
    "\n",
    "        cluster_label = np.zeros((k_KM[i], len(np.unique(y_train))))\n",
    "        for cl in range(cluster_label.shape[0]):\n",
    "            for lb in range(cluster_label.shape[1]):\n",
    "                cluster_label[cl, lb] = np.count_nonzero((y_train_pred==ordering[cl]) & (y_train==lb))\n",
    "        cluster_label = pd.DataFrame(cluster_label, index=np.arange(k_KM[i]), columns=np.unique(y_train))\n",
    "        cluster_labels.append(cluster_label)\n",
    "        \n",
    "        for j in ordering:\n",
    "            ax[m+1].plot(proj[y_train_pred==j, 0], proj[y_train_pred==j, 1], '.', label='cluster '+str(j))\n",
    "        ax[m+1].set_xlabel('TC1')\n",
    "        ax[m+1].set_ylabel('TC2')\n",
    "\n",
    "        m += 2\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.tight_layout()\n",
    "# plt.savefig('Clustering_afterDR_vis.png', bbox_inches='tight', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d9001b-1fee-4143-9b86-3884f271c9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(6, 2, figsize=(12, 25))\n",
    "ax = ax.flatten()\n",
    "\n",
    "ax[0] = sns.heatmap(cluster_labels[0], cmap=cmap, center=0, square=True, linewidths=.5, vmin=0, vmax=2200, cbar_kws={\"shrink\": .5}, ax=ax[0])\n",
    "ax[0].set_xlabel('Labels')\n",
    "ax[0].set_ylabel('Clusters')\n",
    "\n",
    "ax[1] = sns.heatmap(cluster_labels[1], cmap=cmap, center=0, square=True, linewidths=.5, vmin=0, vmax=2200, cbar_kws={\"shrink\": .5}, ax=ax[1])\n",
    "ax[1].set_xlabel('Labels')\n",
    "ax[1].set_ylabel('Clusters')\n",
    "\n",
    "ax[2] = sns.heatmap(cluster_labels[2], cmap=cmap, center=0, square=True, linewidths=.5, vmin=0, vmax=2200, cbar_kws={\"shrink\": .5}, ax=ax[2])\n",
    "ax[2].set_xlabel('Labels')\n",
    "ax[2].set_ylabel('Clusters')\n",
    "\n",
    "ax[3] = sns.heatmap(cluster_labels[3], cmap=cmap, center=0, square=True, linewidths=.5, vmin=0, vmax=2200, cbar_kws={\"shrink\": .5}, ax=ax[3])\n",
    "ax[3].set_xlabel('Labels')\n",
    "ax[3].set_ylabel('Clusters')\n",
    "\n",
    "ax[4] = sns.heatmap(cluster_labels[4], cmap=cmap, center=0, square=True, linewidths=.5, vmin=0, vmax=2200, cbar_kws={\"shrink\": .5}, ax=ax[4])\n",
    "ax[4].set_xlabel('Labels')\n",
    "ax[4].set_ylabel('Clusters')\n",
    "\n",
    "ax[5] = sns.heatmap(cluster_labels[5], cmap=cmap, center=0, square=True, linewidths=.5, vmin=0, vmax=2200, cbar_kws={\"shrink\": .5}, ax=ax[5])\n",
    "ax[5].set_xlabel('Labels')\n",
    "ax[5].set_ylabel('Clusters')\n",
    "\n",
    "ax[6] = sns.heatmap(cluster_labels[6], cmap=cmap, center=0, square=True, linewidths=.5, vmin=0, vmax=160, cbar_kws={\"shrink\": .5}, ax=ax[6])\n",
    "ax[6].set_xlabel('Labels')\n",
    "ax[6].set_ylabel('Clusters')\n",
    "\n",
    "ax[7] = sns.heatmap(cluster_labels[7], cmap=cmap, center=0, square=True, linewidths=.5, vmin=0, vmax=160, cbar_kws={\"shrink\": .5}, ax=ax[7])\n",
    "ax[7].set_xlabel('Labels')\n",
    "ax[7].set_ylabel('Clusters')\n",
    "\n",
    "ax[8] = sns.heatmap(cluster_labels[8], cmap=cmap, center=0, square=True, linewidths=.5, vmin=0, vmax=160, cbar_kws={\"shrink\": .5}, ax=ax[8])\n",
    "ax[8].set_xlabel('Labels')\n",
    "ax[8].set_ylabel('Clusters')\n",
    "\n",
    "ax[9] = sns.heatmap(cluster_labels[9], cmap=cmap, center=0, square=True, linewidths=.5, vmin=0, vmax=160, cbar_kws={\"shrink\": .5}, ax=ax[9])\n",
    "ax[9].set_xlabel('Labels')\n",
    "ax[9].set_ylabel('Clusters')\n",
    "\n",
    "ax[10] = sns.heatmap(cluster_labels[10], cmap=cmap, center=0, square=True, linewidths=.5, vmin=0, vmax=160, cbar_kws={\"shrink\": .5}, ax=ax[10])\n",
    "ax[10].set_xlabel('Labels')\n",
    "ax[10].set_ylabel('Clusters')\n",
    "\n",
    "ax[11] = sns.heatmap(cluster_labels[11], cmap=cmap, center=0, square=True, linewidths=.5, vmin=0, vmax=160, cbar_kws={\"shrink\": .5}, ax=ax[11])\n",
    "ax[11].set_xlabel('Labels')\n",
    "ax[11].set_ylabel('Clusters')\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.tight_layout()\n",
    "plt.savefig('Clustering_afterDR_results.png', bbox_inches='tight', dpi=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
