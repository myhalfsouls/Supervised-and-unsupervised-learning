{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d254b1c-7782-47df-b3b7-5d54c4bd67c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch.callbacks import EpochScoring \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c4eaa4-0317-4722-8d03-44b34d43731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = 'cpu'\n",
    "plt.rcParams.update({'font.size': 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deae68fa-1c9c-482d-903d-ab1a003c70d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define neural network class, code adapted from https://github.com/jlm429/pyperch/blob/master/pyperch/neural/backprop_nn.py\n",
    "\n",
    "class BackpropModule(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_units=10, hidden_layers=1,\n",
    "                 dropout_percent=0, activation=nn.ReLU(), output_activation=nn.Softmax(dim=-1)):\n",
    "        \"\"\"\n",
    "\n",
    "        Initialize the neural network.\n",
    "\n",
    "        PARAMETERS:\n",
    "\n",
    "        input_dim {int}:\n",
    "            Number of features/dimension of the input.  Must be greater than 0.\n",
    "\n",
    "        output_dim {int}:\n",
    "            Number of classes/output dimension of the model. Must be greater than 0.\n",
    "\n",
    "        hidden_units {int}:\n",
    "            Number of hidden units.\n",
    "\n",
    "        hidden_layers {int}:\n",
    "            Number of hidden layers.\n",
    "\n",
    "        dropout_percent {float}:\n",
    "            Probability of an element to be zeroed.\n",
    "\n",
    "        activation {torch.nn.modules.activation}:\n",
    "            Activation function.\n",
    "\n",
    "        output_activation {torch.nn.modules.activation}:\n",
    "            Output activation.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_units = hidden_units\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.dropout = nn.Dropout(dropout_percent)\n",
    "        self.activation = activation\n",
    "        self.output_activation = output_activation\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.device = \"cpu\"\n",
    "\n",
    "        # input layer\n",
    "        self.layers.append(nn.Linear(self.input_dim, self.hidden_units, device=self.device))\n",
    "        # hidden layers\n",
    "        for layer in range(self.hidden_layers):\n",
    "            self.layers.append(nn.Linear(self.hidden_units, self.hidden_units, device=self.device))\n",
    "        # output layer\n",
    "        self.layers.append(nn.Linear(self.hidden_units, self.output_dim, device=self.device))\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        \"\"\"\n",
    "        Recipe for the forward pass.\n",
    "\n",
    "        PARAMETERS:\n",
    "\n",
    "        X {torch.tensor}:\n",
    "            NN input data. Shape (batch_size, input_dim).\n",
    "\n",
    "        RETURNS:\n",
    "\n",
    "        X {torch.tensor}:\n",
    "            NN output data. Shape (batch_size, output_dim).\n",
    "        \"\"\"\n",
    "        X = self.activation(self.layers[0](X))\n",
    "        X = self.dropout(X)\n",
    "        for i in range(self.hidden_layers):\n",
    "            X = self.activation(self.layers[i+1](X))\n",
    "            X = self.dropout(X)\n",
    "        X = self.output_activation(self.layers[self.hidden_layers+1](X))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e4fd9e-0c3c-4c7e-9c07-8b2f5425485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bean = pd.read_excel('.//datasets//bean//Dry_Bean_Dataset.xlsx')\n",
    "\n",
    "X_bean = bean.iloc[:, :-1]\n",
    "y_bean = bean.iloc[:, -1]\n",
    "\n",
    "encoder = preprocessing.LabelEncoder().fit(y_bean)\n",
    "y_bean = encoder.transform(y_bean)\n",
    "\n",
    "X_bean_train, X_bean_test, y_bean_train, y_bean_test = model_selection.train_test_split(X_bean, y_bean, test_size=0.2, random_state=seed, stratify=y_bean)\n",
    "\n",
    "scaler_bean = preprocessing.StandardScaler().fit(X_bean_train)\n",
    "X_bean_train = scaler_bean.transform(X_bean_train)\n",
    "X_bean_test = scaler_bean.transform(X_bean_test)\n",
    "\n",
    "X_train = X_bean_train.astype(np.float32)\n",
    "X_test = X_bean_test.astype(np.float32)\n",
    "y_train = y_bean_train.astype(np.int64)\n",
    "y_test = y_bean_test.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7853d4-7372-4b18-9357-aaf4b8097a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline results\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "NN_base = NeuralNetClassifier(\n",
    "    module=BackpropModule,\n",
    "    module__input_dim=num_features,\n",
    "    module__output_dim=num_classes,\n",
    "    module__hidden_units=16,\n",
    "    module__hidden_layers=1,\n",
    "    module__dropout_percent=0.1,\n",
    "    max_epochs=100,\n",
    "    verbose=0,\n",
    "    callbacks=[EpochScoring(scoring='f1_macro', name='train_acc', on_train=True),],\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    optimizer=optim.SGD,\n",
    "    optimizer__weight_decay=0,\n",
    "    optimizer__momentum=0.9,\n",
    "    lr=0.01,\n",
    "    # Shuffle training data on each epoch\n",
    "    iterator_train__shuffle=True,\n",
    ")\n",
    "\n",
    "t1 = time.time()\n",
    "NN_base.fit(X_train, y_train)\n",
    "t2 = time.time()\n",
    "print('Baseline training time is: {}'.format(t2-t1))\n",
    "\n",
    "results_base = {'train_loss': NN_base.history[:, 'train_loss'], 'valid_loss': NN_base.history[:, 'valid_loss'], 'train_acc': NN_base.history[:, 'train_acc'], 'valid_acc': NN_base.history[:, 'valid_acc']}\n",
    "results_base = pd.DataFrame(results_base)\n",
    "results_base.to_excel('NN_base.xlsx')\n",
    "\n",
    "y_test_prob = NN_base.predict_proba(X_test)\n",
    "y_test_pred = np.argmax(y_test_prob, axis=1)\n",
    "NN_test_acc = metrics.accuracy_score(y_test, y_test_pred)\n",
    "NN_test_recall = metrics.recall_score(y_test, y_test_pred, average='macro')\n",
    "NN_test_f1 = metrics.f1_score(y_test, y_test_pred, average='macro')\n",
    "print('Neural network baseline, test accuracy is: {:.3f}'.format(NN_test_acc))\n",
    "print('Neural network baseline, test recall is: {:.3f}'.format(NN_test_recall))\n",
    "print('Neural network baseline, test f1 score is: {:.3f}'.format(NN_test_f1))\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "cm_plot = metrics.ConfusionMatrixDisplay(cm)\n",
    "cm_plot.plot()\n",
    "cm_plot.figure_.savefig('NN_base_conf.png', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f6ab91-a32f-4cbb-a1ff-d1311261940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline learning curve\n",
    "num_features = X_train.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "NN_base = NeuralNetClassifier(\n",
    "    module=BackpropModule,\n",
    "    module__input_dim=num_features,\n",
    "    module__output_dim=num_classes,\n",
    "    module__hidden_units=16,\n",
    "    module__hidden_layers=1,\n",
    "    module__dropout_percent=0.1,\n",
    "    max_epochs=100,\n",
    "    verbose=0,\n",
    "    callbacks=[EpochScoring(scoring='f1_macro', name='train_acc', on_train=True),],\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    optimizer=optim.SGD,\n",
    "    optimizer__weight_decay=0,\n",
    "    optimizer__momentum=0.9,\n",
    "    lr=0.01,\n",
    "    # Shuffle training data on each epoch\n",
    "    iterator_train__shuffle=True,\n",
    ")\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5)\n",
    "train_sizes, train_scores, val_scores = learning_curve(NN_base, X_train, y_train, train_sizes=np.linspace(0.025, 1, 20), cv=folds, scoring='f1_macro')\n",
    "\n",
    "train_scores_mean = train_scores.mean(axis=1)\n",
    "train_scores_std = train_scores.std(axis=1)\n",
    "val_scores_mean = val_scores.mean(axis=1)\n",
    "val_scores_std = val_scores.std(axis=1)\n",
    "\n",
    "results_lc = {'train_size': train_sizes, 'train_score_mean': train_scores_mean, 'train_score_std': train_scores_std, 'val_score_mean': val_scores_mean, 'val_score_std': val_scores_std}\n",
    "results_lc = pd.DataFrame(results_lc)\n",
    "results_lc.to_excel('NN_base_lc.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551d572a-2967-413d-ad6f-d28494f3c34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DR results\n",
    "\n",
    "n_components = {'PCA': 5, 'ICA': 6, 'RP': 6}\n",
    "folds = StratifiedKFold(n_splits=5)\n",
    "\n",
    "for DR in ['PCA', 'ICA', 'RP']:\n",
    "    if DR == 'PCA':\n",
    "        DR_algo = PCA(n_components=n_components[DR], random_state=seed)\n",
    "        t1 = time.time()\n",
    "        DR_algo.fit(X_train)\n",
    "        X_DR = DR_algo.transform(X_train)\n",
    "        t2 = time.time()\n",
    "        X_DR_test = DR_algo.transform(X_test)\n",
    "    elif DR == 'ICA':\n",
    "        DR_algo = FastICA(n_components=n_components[DR], random_state=seed, max_iter=2000)\n",
    "        t1 = time.time()\n",
    "        DR_algo.fit(X_train)       \n",
    "        X_DR = DR_algo.transform(X_train)\n",
    "        t2 = time.time()\n",
    "        X_DR_test = DR_algo.transform(X_test)\n",
    "    elif DR == 'RP':\n",
    "        DR_algo = GaussianRandomProjection(n_components=n_components[DR], random_state=seed)\n",
    "        t1 = time.time()\n",
    "        DR_algo.fit(X_train)\n",
    "        X_DR = DR_algo.transform(X_train)\n",
    "        t2 = time.time()\n",
    "        X_DR_test = DR_algo.transform(X_test)\n",
    "\n",
    "    num_features = X_DR.shape[1]\n",
    "    num_classes = len(np.unique(y_train))\n",
    "\n",
    "    NN_DR = NeuralNetClassifier(\n",
    "        module=BackpropModule,\n",
    "        module__input_dim=num_features,\n",
    "        module__output_dim=num_classes,\n",
    "        module__hidden_units=16,\n",
    "        module__hidden_layers=1,\n",
    "        module__dropout_percent=0.1,\n",
    "        max_epochs=100,\n",
    "        verbose=0,\n",
    "        callbacks=[EpochScoring(scoring='f1_macro', name='train_acc', on_train=True),],\n",
    "        criterion=nn.CrossEntropyLoss,\n",
    "        optimizer=optim.SGD,\n",
    "        optimizer__weight_decay=0,\n",
    "        optimizer__momentum=0.9,\n",
    "        lr=0.01,\n",
    "        # Shuffle training data on each epoch\n",
    "        iterator_train__shuffle=True,\n",
    "    )\n",
    "\n",
    "    t3 = time.time()\n",
    "    NN_DR.fit(X_DR, y_train)\n",
    "    t4 = time.time()\n",
    "    \n",
    "    print('NN with {} training time is: {} + {} (DR time + NN fit time)'.format(DR, t2-t1, t4-t3))\n",
    "\n",
    "    results_DR = {'train_loss': NN_DR.history[:, 'train_loss'], 'valid_loss': NN_DR.history[:, 'valid_loss'], 'train_acc': NN_DR.history[:, 'train_acc'], 'valid_acc': NN_DR.history[:, 'valid_acc']}\n",
    "    results_DR = pd.DataFrame(results_DR)\n",
    "    results_DR.to_excel('NN_{}.xlsx'.format(DR))\n",
    "\n",
    "    y_test_prob = NN_DR.predict_proba(X_DR_test)\n",
    "    y_test_pred = np.argmax(y_test_prob, axis=1)\n",
    "    NN_test_acc = metrics.accuracy_score(y_test, y_test_pred)\n",
    "    NN_test_recall = metrics.recall_score(y_test, y_test_pred, average='macro')\n",
    "    NN_test_f1 = metrics.f1_score(y_test, y_test_pred, average='macro')\n",
    "    print('Neural network after {}, test accuracy is: {:.3f}'.format(DR, NN_test_acc))\n",
    "    print('Neural network after {}, test recall is: {:.3f}'.format(DR, NN_test_recall))\n",
    "    print('Neural network after {}, test f1 score is: {:.3f}'.format(DR, NN_test_f1))\n",
    "\n",
    "    cm = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "    cm_plot = metrics.ConfusionMatrixDisplay(cm)\n",
    "    cm_plot.plot()\n",
    "    cm_plot.figure_.savefig('NN_{}_conf.png'.format(DR), dpi=500)\n",
    "\n",
    "    train_sizes, train_scores, val_scores = learning_curve(NN_DR, X_DR, y_train, train_sizes=np.linspace(0.025, 1, 20), cv=folds, scoring='f1_macro')\n",
    "\n",
    "    train_scores_mean = train_scores.mean(axis=1)\n",
    "    train_scores_std = train_scores.std(axis=1)\n",
    "    val_scores_mean = val_scores.mean(axis=1)\n",
    "    val_scores_std = val_scores.std(axis=1)\n",
    "    \n",
    "    results_lc = {'train_size': train_sizes, 'train_score_mean': train_scores_mean, 'train_score_std': train_scores_std, 'val_score_mean': val_scores_mean, 'val_score_std': val_scores_std}\n",
    "    results_lc = pd.DataFrame(results_lc)\n",
    "    results_lc.to_excel('NN_{}_lc.xlsx'.format(DR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac081da-0516-4a02-b659-9a017e7e6226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline alt\n",
    "\n",
    "redundant = ['Perimeter', 'MajorAxisLength', 'Area', 'ConvexArea', 'EquivDiameter', 'MinorAxisLength', 'Eccentricity', 'Compactness', 'ShapeFactor2', 'ShapeFactor3']\n",
    "col_idx = [X_bean.columns.get_loc(c) for c in redundant]\n",
    "retain = np.delete(np.arange(X_train.shape[1]), col_idx)\n",
    "X_train_redu = X_train[:, retain]\n",
    "X_test_redu = X_test[:, retain]\n",
    "\n",
    "num_features = X_train_redu.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "NN_alt = NeuralNetClassifier(\n",
    "    module=BackpropModule,\n",
    "    module__input_dim=num_features,\n",
    "    module__output_dim=num_classes,\n",
    "    module__hidden_units=16,\n",
    "    module__hidden_layers=1,\n",
    "    module__dropout_percent=0.1,\n",
    "    max_epochs=100,\n",
    "    verbose=0,\n",
    "    callbacks=[EpochScoring(scoring='f1_macro', name='train_acc', on_train=True),],\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    optimizer=optim.SGD,\n",
    "    optimizer__weight_decay=0,\n",
    "    optimizer__momentum=0.9,\n",
    "    lr=0.01,\n",
    "    # Shuffle training data on each epoch\n",
    "    iterator_train__shuffle=True,\n",
    ")\n",
    "\n",
    "t1 = time.time()\n",
    "NN_alt.fit(X_train_redu, y_train)\n",
    "t2 = time.time()\n",
    "\n",
    "print('Baseline training time is: {}'.format(t2-t1))\n",
    "\n",
    "results_base = {'train_loss': NN_alt.history[:, 'train_loss'], 'valid_loss': NN_alt.history[:, 'valid_loss'], 'train_acc': NN_alt.history[:, 'train_acc'], 'valid_acc': NN_alt.history[:, 'valid_acc']}\n",
    "results_base = pd.DataFrame(results_base)\n",
    "results_base.to_excel('NN_alt.xlsx')\n",
    "\n",
    "y_test_prob = NN_alt.predict_proba(X_test_redu)\n",
    "y_test_pred = np.argmax(y_test_prob, axis=1)\n",
    "NN_test_acc = metrics.accuracy_score(y_test, y_test_pred)\n",
    "NN_test_recall = metrics.recall_score(y_test, y_test_pred, average='macro')\n",
    "NN_test_f1 = metrics.f1_score(y_test, y_test_pred, average='macro')\n",
    "print('Neural network baseline, test accuracy is: {:.3f}'.format(NN_test_acc))\n",
    "print('Neural network baseline, test recall is: {:.3f}'.format(NN_test_recall))\n",
    "print('Neural network baseline, test f1 score is: {:.3f}'.format(NN_test_f1))\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5)\n",
    "train_sizes, train_scores, val_scores = learning_curve(NN_alt, X_train_redu, y_train, train_sizes=np.linspace(0.025, 1, 20), cv=folds, scoring='f1_macro')\n",
    "\n",
    "train_scores_mean = train_scores.mean(axis=1)\n",
    "train_scores_std = train_scores.std(axis=1)\n",
    "val_scores_mean = val_scores.mean(axis=1)\n",
    "val_scores_std = val_scores.std(axis=1)\n",
    "\n",
    "results_lc = {'train_size': train_sizes, 'train_score_mean': train_scores_mean, 'train_score_std': train_scores_std, 'val_score_mean': val_scores_mean, 'val_score_std': val_scores_std}\n",
    "results_lc = pd.DataFrame(results_lc)\n",
    "results_lc.to_excel('NN_alt_lc.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdd3126-a90e-4d2b-8393-2d8f5e1b9c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = pd.read_excel('NN_base.xlsx', index_col=0)\n",
    "alt = pd.read_excel('NN_alt.xlsx', index_col=0)\n",
    "pca = pd.read_excel('NN_PCA.xlsx', index_col=0)\n",
    "ica = pd.read_excel('NN_ICA.xlsx', index_col=0)\n",
    "rp = pd.read_excel('NN_RP.xlsx', index_col=0)\n",
    "\n",
    "base_lc = pd.read_excel('NN_base_lc.xlsx', index_col=0)\n",
    "alt_lc = pd.read_excel('NN_alt_lc.xlsx', index_col=0)\n",
    "pca_lc = pd.read_excel('NN_PCA_lc.xlsx', index_col=0)\n",
    "ica_lc = pd.read_excel('NN_ICA_lc.xlsx', index_col=0)\n",
    "rp_lc = pd.read_excel('NN_RP_lc.xlsx', index_col=0)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5.5))\n",
    "ax = ax.flatten()\n",
    "ax[0].plot(base.loc[:, 'valid_acc'], label='baseline')\n",
    "ax[0].plot(alt.loc[:, 'valid_acc'], label='covar')\n",
    "ax[0].plot(pca.loc[:, 'valid_acc'], label='after PCA')\n",
    "ax[0].plot(ica.loc[:, 'valid_acc'], label='after ICA')\n",
    "ax[0].plot(rp.loc[:, 'valid_acc'], label='after RP')\n",
    "ax[0].set_ylim([0, 1.05])\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Validation f1 score')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(base_lc.loc[:, 'train_size'], base_lc.loc[:, 'val_score_mean'], label='baseline')\n",
    "ax[1].plot(alt_lc.loc[:, 'train_size'], alt_lc.loc[:, 'val_score_mean'], label='covar')\n",
    "ax[1].plot(pca_lc.loc[:, 'train_size'], pca_lc.loc[:, 'val_score_mean'], label='after PCA')\n",
    "ax[1].plot(ica_lc.loc[:, 'train_size'], ica_lc.loc[:, 'val_score_mean'], label='after ICA')\n",
    "ax[1].plot(rp_lc.loc[:, 'train_size'], rp_lc.loc[:, 'val_score_mean'], label='after RP')\n",
    "ax[1].set_ylim([0.05, 1.05])\n",
    "ax[1].set_xlabel('Training size')\n",
    "ax[1].set_ylabel('Validation f1 score')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.tight_layout()\n",
    "# plt.savefig('NN_wDR.png', dpi=500, bbox_inches='tight')\n",
    "\n",
    "window = 15\n",
    "thresh = 0.001\n",
    "\n",
    "base_window = base['valid_acc'].rolling(window=window).mean()\n",
    "print('Base case converges at epoch {}'.format((base_window.diff() < thresh).idxmax()))\n",
    "alt_window = alt['valid_acc'].rolling(window=window).mean()\n",
    "print('Covar case converges at epoch {}'.format((alt_window.diff() < thresh).idxmax()))\n",
    "pca_window = pca['valid_acc'].rolling(window=window).mean()\n",
    "print('PCA case converges at epoch {}'.format((pca_window.diff() < thresh).idxmax()))\n",
    "ica_window = ica['valid_acc'].rolling(window=window).mean()\n",
    "print('ICA case converges at epoch {}'.format((ica_window.diff() < thresh).idxmax()))\n",
    "rp_window = rp['valid_acc'].rolling(window=window).mean()\n",
    "print('RP case converges at epoch {}'.format((rp_window.diff() < thresh).idxmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d85204-a889-4f85-bdfe-ddb789ac2478",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = {'GMM': 5, 'KM': 5}\n",
    "folds = StratifiedKFold(n_splits=5)\n",
    "\n",
    "for C in ['GMM', 'KM']:\n",
    "    if C == 'GMM':\n",
    "        gmm = GaussianMixture(n_components=n_clusters['GMM'], random_state=seed, max_iter=500, n_init=25, covariance_type='full', tol=0.0001)\n",
    "        gmm.fit(X_train)\n",
    "        clusters = gmm.predict(X_train)\n",
    "        clusters_test = gmm.predict(X_test)\n",
    "        onehot_train = preprocessing.OneHotEncoder(sparse_output=False)\n",
    "        clusters = onehot_train.fit_transform(clusters.reshape(-1, 1))\n",
    "        onehot_test = preprocessing.OneHotEncoder(sparse_output=False)\n",
    "        clusters_test = onehot_test.fit_transform(clusters_test.reshape(-1, 1))\n",
    "        X_train_add = np.concatenate([X_train, clusters], axis=1).astype(np.float32)\n",
    "        X_test_add = np.concatenate([X_test, clusters_test], axis=1).astype(np.float32)\n",
    "    elif C == 'KM':\n",
    "        km = KMeans(n_clusters=n_clusters['KM'], max_iter=500, random_state=seed, n_init=500)\n",
    "        km.fit(X_train)\n",
    "        clusters = km.predict(X_train).reshape(-1, 1)\n",
    "        clusters_test = km.predict(X_test).reshape(-1, 1)\n",
    "        # sil_values = metrics.silhouette_samples(X_train, clusters)\n",
    "        # sil_values_test = metrics.silhouette_samples(X_test, clusters_test)\n",
    "        onehot_train = preprocessing.OneHotEncoder(sparse_output=False)\n",
    "        clusters = onehot_train.fit_transform(clusters.reshape(-1, 1))\n",
    "        onehot_test = preprocessing.OneHotEncoder(sparse_output=False)\n",
    "        clusters_test = onehot_test.fit_transform(clusters_test.reshape(-1, 1))\n",
    "        X_train_add = np.concatenate([X_train, clusters], axis=1).astype(np.float32)\n",
    "        X_test_add = np.concatenate([X_test, clusters_test], axis=1).astype(np.float32)\n",
    "        \n",
    "    num_features = X_train_add.shape[1]\n",
    "    num_classes = len(np.unique(y_train))\n",
    "\n",
    "    NN_C = NeuralNetClassifier(\n",
    "        module=BackpropModule,\n",
    "        module__input_dim=num_features,\n",
    "        module__output_dim=num_classes,\n",
    "        module__hidden_units=16,\n",
    "        module__hidden_layers=1,\n",
    "        module__dropout_percent=0.1,\n",
    "        max_epochs=100,\n",
    "        verbose=0,\n",
    "        callbacks=[EpochScoring(scoring='f1_macro', name='train_acc', on_train=True),],\n",
    "        criterion=nn.CrossEntropyLoss,\n",
    "        optimizer=optim.SGD,\n",
    "        optimizer__weight_decay=0,\n",
    "        optimizer__momentum=0.9,\n",
    "        lr=0.01,\n",
    "        # Shuffle training data on each epoch\n",
    "        iterator_train__shuffle=True,\n",
    "    )\n",
    "\n",
    "    t1 = time.time()\n",
    "    NN_C.fit(X_train_add, y_train)\n",
    "    t2 = time.time()\n",
    "    \n",
    "    print('NN with {} training time is: {}'.format(C, t2-t1))\n",
    "\n",
    "    results_C = {'train_loss': NN_C.history[:, 'train_loss'], 'valid_loss': NN_C.history[:, 'valid_loss'], 'train_acc': NN_C.history[:, 'train_acc'], 'valid_acc': NN_C.history[:, 'valid_acc']}\n",
    "    results_C = pd.DataFrame(results_C)\n",
    "    results_C.to_excel('NN_{}.xlsx'.format(C))\n",
    "\n",
    "    y_test_prob = NN_C.predict_proba(X_test_add)\n",
    "    y_test_pred = np.argmax(y_test_prob, axis=1)\n",
    "    NN_test_acc = metrics.accuracy_score(y_test, y_test_pred)\n",
    "    NN_test_recall = metrics.recall_score(y_test, y_test_pred, average='macro')\n",
    "    NN_test_f1 = metrics.f1_score(y_test, y_test_pred, average='macro')\n",
    "    print('Neural network after {}, test accuracy is: {:.3f}'.format(C, NN_test_acc))\n",
    "    print('Neural network after {}, test recall is: {:.3f}'.format(C, NN_test_recall))\n",
    "    print('Neural network after {}, test f1 score is: {:.3f}'.format(C, NN_test_f1))\n",
    "\n",
    "    cm = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "    cm_plot = metrics.ConfusionMatrixDisplay(cm)\n",
    "    cm_plot.plot()\n",
    "    cm_plot.figure_.savefig('NN_{}_conf.png'.format(C), dpi=500)\n",
    "\n",
    "    train_sizes, train_scores, val_scores = learning_curve(NN_C, X_train_add, y_train, train_sizes=np.linspace(0.025, 1, 20), cv=folds, scoring='f1_macro')\n",
    "\n",
    "    train_scores_mean = train_scores.mean(axis=1)\n",
    "    train_scores_std = train_scores.std(axis=1)\n",
    "    val_scores_mean = val_scores.mean(axis=1)\n",
    "    val_scores_std = val_scores.std(axis=1)\n",
    "    \n",
    "    results_C_lc = {'train_size': train_sizes, 'train_score_mean': train_scores_mean, 'train_score_std': train_scores_std, 'val_score_mean': val_scores_mean, 'val_score_std': val_scores_std}\n",
    "    results_C_lc = pd.DataFrame(results_C_lc)\n",
    "    results_C_lc.to_excel('NN_{}_lc.xlsx'.format(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f9ca49-9d3f-4487-9d83-219f9c4cc453",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = pd.read_excel('NN_base.xlsx', index_col=0)\n",
    "gmm = pd.read_excel('NN_GMM.xlsx', index_col=0)\n",
    "km = pd.read_excel('NN_KM.xlsx', index_col=0)\n",
    "\n",
    "base_lc = pd.read_excel('NN_base_lc.xlsx', index_col=0)\n",
    "gmm_lc = pd.read_excel('NN_GMM_lc.xlsx', index_col=0)\n",
    "km_lc = pd.read_excel('NN_KM_lc.xlsx', index_col=0)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5.5))\n",
    "ax = ax.flatten()\n",
    "ax[0].plot(base.loc[:, 'valid_acc'], label='baseline')\n",
    "ax[0].plot(gmm.loc[:, 'valid_acc'], label='after GMM')\n",
    "ax[0].plot(km.loc[:, 'valid_acc'], label='after KM')\n",
    "ax[0].set_ylim([0.05, 1.05])\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Validation f1 score')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(base_lc.loc[:, 'train_size'], base_lc.loc[:, 'val_score_mean'], label='baseline')\n",
    "ax[1].plot(gmm_lc.loc[:, 'train_size'], gmm_lc.loc[:, 'val_score_mean'], label='after GMM')\n",
    "ax[1].plot(km_lc.loc[:, 'train_size'], km_lc.loc[:, 'val_score_mean'], label='after KM')\n",
    "ax[1].set_ylim([0.05, 1.05])\n",
    "ax[1].set_xlabel('Training size')\n",
    "ax[1].set_ylabel('Validation f1 score')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.tight_layout()\n",
    "# plt.savefig('NN_wC.png', dpi=500, bbox_inches='tight')\n",
    "\n",
    "base_window = base['valid_acc'].rolling(window=window).mean()\n",
    "print('Base case converges at epoch {}'.format((base_window.diff() < thresh).idxmax()))\n",
    "gmm_window = gmm['valid_acc'].rolling(window=window).mean()\n",
    "print('GMM case converges at epoch {}'.format((gmm_window.diff() < thresh).idxmax()))\n",
    "km_window = km['valid_acc'].rolling(window=window).mean()\n",
    "print('KM case converges at epoch {}'.format((km_window.diff() < thresh).idxmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afee1157-6018-436d-88d4-ddec0a9908c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
